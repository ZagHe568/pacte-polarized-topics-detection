{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rising-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('data/df_news.csv')\n",
    "data = df['summary'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "distributed-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences, min_len=2, max_len=15):\n",
    "    # tokenize words\n",
    "    import gensim\n",
    "    for sentence in sentences:\n",
    "        yield gensim.utils.simple_preprocess(str(sentence), deacc=True, \n",
    "                                             min_len=min_len, max_len=max_len)  # deacc=True removes punctuations\n",
    "\n",
    "\n",
    "def remove_stopwords(texts, default='english', extensions=None):\n",
    "    # nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = []\n",
    "    if default is not None:\n",
    "        stop_words.extend(stopwords.words(default))\n",
    "    if extensions is not None:\n",
    "        stop_words.extend(extensions)\n",
    "    import gensim\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def make_bigrams(data_words):\n",
    "    import gensim\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshld fewer phrases\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in data_words], bigram, bigram_mod\n",
    "\n",
    "\n",
    "def make_trigrams(data_words, bigram, bigram_mod):\n",
    "    import gensim\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in data_words], trigram, trigram_mod\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    '''\n",
    "    Lemmatization for LDA topic modeling.\n",
    "    '''\n",
    "    import spacy\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        # do lemmatization and only keep the types of tokens in allowed_postags\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def lemmatization2(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    '''\n",
    "    Lemmatization for BERT. \n",
    "    Although BERT has its own tokenizer, we need match the words for BERT and LDA.\n",
    "    '''\n",
    "    import spacy\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        # for tokens whose types in allowed_postages do lemmatization otherwise keep the original form\n",
    "        texts_out.append([str(token.lemma_) if token.pos_ in allowed_postags else token for token in doc])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def lemmatization3(texts):\n",
    "    '''\n",
    "    Lemmatization for leave-out estimator\n",
    "    '''\n",
    "    import spacy\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        # for all tokens do lemmatization and keep all tokens\n",
    "        texts_out.append([str(token.lemma_) for token in doc])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "integral-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_corpus(data_words):\n",
    "    import gensim.corpora as corpora\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_words\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    return corpus, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caroline-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lda(data):\n",
    "    import re\n",
    "    \n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    # tokenize words and clean-up text\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # remove stop words\n",
    "    # need to remove the news source names\n",
    "    data_words_nostops = remove_stopwords(data_words, \n",
    "                                          extensions=['from', 'subject', 're', 'edu', \n",
    "                                                       'use', 'rt', 'cnn', 'fox', 'huffington', 'breitbart'])\n",
    "\n",
    "    # form bigrams\n",
    "    data_words_bigrams, _, _ = make_bigrams(data_words_nostops)\n",
    "\n",
    "    #  do lemmatization keeping only noun, adj, vb, adv, propnoun\n",
    "    # other tokens are not useful for topic modeling\n",
    "    data_lematized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN'])\n",
    "    \n",
    "    corpus, id2word = create_dict_corpus(data_lematized)\n",
    "\n",
    "    return data_lematized, corpus, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "foreign-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed_lda, corpus_lda, id2word_lda = preprocessing_lda(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "governing-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(texts_processed_lda, open('data/texts_processed_lda.pkl', 'wb'))\n",
    "pickle.dump((corpus_lda, id2word_lda), open('data/corpus_lda.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "single-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_bert(data):\n",
    "    import re\n",
    "    \n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "    # tokenize words and clean-up text\n",
    "    data_words = list(sent_to_words(data,min_len=1, max_len=30))\n",
    "\n",
    "    # remove stop words\n",
    "    data_words_nostops = remove_stopwords(data_words, default=None,\n",
    "                                          extensions=['cnn', 'fox', 'huffington', 'breitbart'])\n",
    "\n",
    "    # form bigrams\n",
    "    data_words_bigrams, _, _ = make_bigrams(data_words)\n",
    "\n",
    "    #  do lemmatization for only noun, adj, vb, adv propnoun, following the lemmatization for LDA\n",
    "    #  keep the others which will be used as context\n",
    "    data_lematized = lemmatization2(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN'])\n",
    "    \n",
    "    return data_lematized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "centered-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processed_bert = preprocessing_bert(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "married-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processed_bert = [[str(x) for x in y] for y in text_processed_bert]\n",
    "pickle.dump(text_processed_bert, open('data/texts_processed_bert.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pending-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reese_witherspoon'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed_bert[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thirty-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_lo(data):\n",
    "    import re\n",
    "    \n",
    "    # Remove Emails\n",
    "    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "    # Remove new line characters\n",
    "    data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "    \n",
    "    # Remove distracting single quotes\n",
    "    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "    # tokenize words and clean-up text\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # remove stop words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # form bigrams\n",
    "    data_words_bigrams, _, _ = make_bigrams(data_words_nostops)\n",
    "\n",
    "    #  do lemmatization for only noun, adj, vb, adv, and keep all of them\n",
    "    data_lematized = lemmatization3(data_words_bigrams)\n",
    "    corpus, id2word = create_dict_corpus(data_lematized)\n",
    "    \n",
    "    return data_lematized, corpus, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sunset-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processed_lo, corpus_lo, id2word_lo = preprocessing_lo(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "institutional-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(text_processed_lo, open('data/texts_processed_lo.pkl', 'wb'))\n",
    "pickle.dump((corpus_lo, id2word_lo), open('data/corpus_lo.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
